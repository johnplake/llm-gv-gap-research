% V2G / Generator-Validator Gap Research - Bibliography
% Compiled February 2026

%% ========================================
%% CORE PAPERS
%% ========================================

@inproceedings{rodriguez2025rankalign,
  title={RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models},
  author={Rodriguez, Juan Diego and Ding, Wenxuan and Erk, Katrin and Durrett, Greg},
  booktitle={Conference on Language Modeling (COLM)},
  year={2025},
  url={https://arxiv.org/abs/2504.11381}
}

@inproceedings{gekhman2025insideout,
  title={Inside-Out: Hidden Factual Knowledge in LLMs},
  author={Gekhman, Zorik and Ben David, Eyal and Orgad, Hadas and Ofek, Eran and Belinkov, Yonatan and Szpektor, Idan and Herzig, Jonathan and Reichart, Roi},
  booktitle={Conference on Language Modeling (COLM)},
  year={2025},
  url={https://arxiv.org/abs/2503.15299}
}

@inproceedings{west2024generative,
  title={The Generative AI Paradox: "What It Can Create, It May Not Understand"},
  author={West, Peter and Lu, Ximing and Dziri, Nouha and Brahman, Faeze and Li, Linjie and Hwang, Jena D and Jiang, Liwei and Fisher, Jillian and Ravichander, Abhilasha and Chandu, Khyathi and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2311.00059}
}

@article{kadavath2022language,
  title={Language Models (Mostly) Know What They Know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022},
  url={https://arxiv.org/abs/2207.05221}
}

%% ========================================
%% GENERATOR-VALIDATOR INCONSISTENCY
%% ========================================

@article{saunders2022self,
  title={Self-critiquing Models for Assisting Human Evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022},
  url={https://cdn.openai.com/papers/critiques.pdf}
}

@inproceedings{li2024benchmarking,
  title={Benchmarking and Improving Generator-Validator Consistency of Language Models},
  author={Li, Xiang Lisa and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2310.01846}
}

@article{huang2024selfincorrect,
  title={Self-[In]Correct: LLMs Struggle with Discriminating Self-Generated Responses},
  author={Huang, Jie and others},
  journal={arXiv preprint arXiv:2404.04298},
  year={2024},
  url={https://arxiv.org/abs/2404.04298}
}

@article{saadfalakon2025weaver,
  title={Shrinking the Generation-Verification Gap with Weak Verifiers},
  author={Saad-Falcon, Jon and Buchanan, E Kelly and Chen, Mayee F and others},
  journal={arXiv preprint arXiv:2506.18203},
  year={2025},
  url={https://arxiv.org/abs/2506.18203}
}

%% ========================================
%% KNOWLEDGE IN LLMS / PROBING
%% ========================================

@inproceedings{orgad2025llmsknow,
  title={LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations},
  author={Orgad, Hadas and Toker, Michael and Gekhman, Zorik and Reichart, Roi and Szpektor, Idan and Kotek, Hadas and Belinkov, Yonatan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  url={https://arxiv.org/abs/2410.02707}
}

@inproceedings{azaria2023internal,
  title={The Internal State of an LLM Knows When It's Lying},
  author={Azaria, Amos and Mitchell, Tom},
  booktitle={Findings of EMNLP},
  year={2023},
  url={https://arxiv.org/abs/2304.13734}
}

@inproceedings{marks2024geometry,
  title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
  author={Marks, Samuel and Tegmark, Max},
  booktitle={Conference on Language Modeling (COLM)},
  year={2024},
  url={https://arxiv.org/abs/2310.06824}
}

@inproceedings{burns2023discovering,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://arxiv.org/abs/2212.03827}
}

%% ========================================
%% REPRESENTATION ENGINEERING
%% ========================================

@article{zou2023representation,
  title={Representation Engineering: A Top-Down Approach to AI Transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023},
  url={https://arxiv.org/abs/2310.01405}
}

@inproceedings{li2023inference,
  title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://arxiv.org/abs/2306.03341}
}

@article{ghandeharioun2024patchscopes,
  title={Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models},
  author={Ghandeharioun, Asma and others},
  journal={arXiv preprint arXiv:2401.06102},
  year={2024},
  url={https://arxiv.org/abs/2401.06102}
}

%% ========================================
%% PREFERENCE LEARNING / ALIGNMENT
%% ========================================

@inproceedings{ouyang2022training,
  title={Training Language Models to Follow Instructions with Human Feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  url={https://arxiv.org/abs/2203.02155}
}

@inproceedings{rafailov2023direct,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://arxiv.org/abs/2305.18290}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022},
  url={https://arxiv.org/abs/2212.08073}
}

@inproceedings{meng2024simpo,
  title={SimPO: Simple Preference Optimization with a Reference-Free Reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024},
  url={https://arxiv.org/abs/2405.14734}
}

@article{ethayarajh2024kto,
  title={KTO: Model Alignment as Prospect Theoretic Optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@inproceedings{liu2025lipo,
  title={LiPO: Listwise Preference Optimization through Learning-to-Rank},
  author={Liu, Tianqi and Qin, Zhen and Yu, Junru and Wu, Jing and Zhu, Jiaming and Zhao, Cong and Yan, Ming and others},
  booktitle={Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2025},
  url={https://arxiv.org/abs/2402.01878}
}

@inproceedings{yuan2024selfrewarding,
  title={Self-Rewarding Language Models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024},
  url={https://arxiv.org/abs/2401.10020}
}

@inproceedings{gao2023scaling,
  title={Scaling Laws for Reward Model Overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023},
  url={https://arxiv.org/abs/2210.10760}
}

%% ========================================
%% CALIBRATION AND UNCERTAINTY
%% ========================================

@inproceedings{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}

@inproceedings{xiong2024can,
  title={Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2306.13063}
}

@article{geng2024survey,
  title={A Survey of Confidence Estimation and Calibration in Large Language Models},
  author={Geng, Jiahui and Cai, Fengyu and Wang, Yuxia and Kober, Heinz and Buntine, Wray and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2311.08298},
  year={2024},
  url={https://arxiv.org/abs/2311.08298}
}

@inproceedings{banda2024llms,
  title={Large Language Models Must Be Taught to Know What They Don't Know},
  author={Banda, Sanyam and Gong, Yao and Lin, Bill Yuchen and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

%% ========================================
%% SELF-CONSISTENCY AND REASONING
%% ========================================

@inproceedings{wang2023selfconsistency,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://arxiv.org/abs/2203.11171}
}

@inproceedings{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  url={https://arxiv.org/abs/2201.11903}
}

@article{huang2022llms,
  title={Large Language Models Can Self-Improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022},
  url={https://arxiv.org/abs/2210.11610}
}

@article{huang2024selfcorrection,
  title={When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs},
  author={Huang, Ryo and others},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  year={2024}
}

%% ========================================
%% INSTRUCTION FOLLOWING
%% ========================================

@inproceedings{yao2024collie,
  title={COLLIE: Systematic Construction of Constrained Text Generation Tasks},
  author={Yao, Shunyu and Chen, Howard and Hanjie, Austin W and Yang, Runzhe and Narasimhan, Karthik},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2307.08689}
}

@inproceedings{zeng2024llmbar,
  title={Evaluating Large Language Models at Evaluating Instruction Following},
  author={Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2310.07641}
}

@article{zhou2023ifeval,
  title={Instruction-Following Evaluation for Large Language Models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023},
  url={https://arxiv.org/abs/2311.07911}
}

@inproceedings{qin2024infobench,
  title={InFoBench: Evaluating Instruction Following Ability in Large Language Models},
  author={Qin, Yiwei and others},
  booktitle={Findings of ACL},
  year={2024}
}

@inproceedings{zheng2023judging,
  title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://arxiv.org/abs/2306.05685}
}

%% ========================================
%% DECODING METHODS
%% ========================================

@inproceedings{chuang2024dola,
  title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2309.03883}
}

@inproceedings{li2023contrastive,
  title={Contrastive Decoding: Open-ended Text Generation as Optimization},
  author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={Association for Computational Linguistics (ACL)},
  year={2023}
}

@article{zhang2023alleviating,
  title={Alleviating Hallucinations of Large Language Models through Induced Hallucinations},
  author={Zhang, Yijiang and Ren, Qitong and others},
  journal={arXiv preprint arXiv:2312.15710},
  year={2023},
  url={https://arxiv.org/abs/2312.15710}
}

%% ========================================
%% BENCHMARKS
%% ========================================

@inproceedings{lin2022truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Association for Computational Linguistics (ACL)},
  year={2022},
  url={https://arxiv.org/abs/2109.07958}
}

%% ========================================
%% INSTRUCTION TUNING
%% ========================================

@article{wei2022flan,
  title={Scaling Instruction-Finetuned Language Models},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022},
  url={https://arxiv.org/abs/2210.11416}
}

%% ========================================
%% LEARNING TO RANK (FOUNDATIONS)
%% ========================================

@inproceedings{cao2007learning,
  title={Learning to Rank: From Pairwise Approach to Listwise Approach},
  author={Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2007}
}

@article{burges2010ranknet,
  title={From RankNet to LambdaRank to LambdaMART: An Overview},
  author={Burges, Christopher JC},
  journal={Learning},
  volume={11},
  number={23-581},
  pages={81},
  year={2010}
}
