# NND (Laban et al., EMNLP 2022) — dataset acquisition + global stats (V2G view)

This note summarizes what we currently have locally from the **Near-Negative Distinction (NND)** ecosystem and computes **global / distributional stats** in a way that is directly relevant to V2G training/evaluation.

**Paper:** Philippe Laban et al. (2022), *Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets* (EMNLP 2022)
- arXiv: https://arxiv.org/abs/2205.06871
- Code repo: https://github.com/salesforce/nnd_evaluation

**V2G goal:** for each prompt/group, obtain *many* candidate completions with a natural positive/negative distinction.

---

## Where the data/code is in this repo/workspace

- Code repo cloned (read-only, upstream):
  - `Projects/v2g/datasets/candidates/nnd_evaluation/`

- Data downloaded for analysis (raw files):
  - `Projects/v2g/datasets/candidates/nnd_data/`

---

## Conventions (V2G view)

For each sub-dataset:
- **Prompt** = the shared input that defines a slice/group (e.g., MT source segment)
- **Candidates** = multiple system outputs for the same prompt
- **POS vs NEG** = a natural label from human evaluation annotations

---

## Summary table (per sub-dataset)

| Sub-dataset | Natural prompt grouping | # prompts | # candidates / prompt (min / p50 / p90 / max) | POS share overall | POS per prompt (p50) | NEG per prompt (p50) | POS definition |
|---|---|---:|---:|---:|---:|---:|---|
| **MT: WMT21 MQM EN→DE** (`mqm_newstest2021_ende.tsv`) | (doc_id, seg_id, source) | **1180** | **1 / 2 / 17 / 17** | **0.588** | **2** | **1** | system has only `No-error` (category & severity) |
| **QA: Challenge300** (`challenge300-outputs.tsv`) | (id, question) | **300** | **1 / 5 / 5 / 5** | **0.689** | **4** | **1** | credit==1 is POS, credit==0 is NEG |
| **QGen: QuizDesign** (`quiz_design_groups.jsonl`) | (group_id, answer_span) | **452** | **1 / 6 / 7 / 7** | **0.460** | **2** | **3** | reason==`No error` is POS |
| **Summarization: “News Summarization in era of GPT‑3” (CNN)** (`cnn_human.json`) | doc_id | **100** | **3 / 3 / 3 / 3** | **0.377** | **1** | *(see note)* | POS = top voted summary (best/worst votes) |
| **Summarization: same (BBC)** (`bbc_human.json`) | doc_id | **100** | **3 / 3 / 3 / 3** | **0.377** | **1** | *(see note)* | same as above |
| **Summarization: same (Keyword)** (`keyword_human.json`) | — | **(not parsed yet)** | — | — | — | — | file schema differs (see below) |
| **Summarization: FRANK (CNNDM subset, sentence-level factuality)** (`frank/human_annotations_sentence.json`) | hash | **175** | **5 / 5 / 5 / 5** | **0.563** | **3** | **2** | error_type==`NoE` is POS; otherwise NEG (majority error type) |
| **Summarization: SummEval (CNNDM subset; 16 model outputs + expert+turker ratings)** (`summeval/model_annotations.aligned.jsonl`) | cnndm_id (= `id`, e.g. `dm-test-<hash>`) | **100** | **16 / 16 / 16 / 16** | *(varies by dimension)* | *(varies)* | *(varies)* | POS by **expert-majority-of-5** per dimension (see below) |

**Notes:**
- The GPT‑3 summarization annotation sets (CNN/BBC) have exactly **3 candidates per prompt** (gpt3/t0/brio). POS was defined as “max score” where score is computed from annotators choosing best (+1) and worst (−1). This is not strictly a symmetric POS/NEG split because ties can occur.

---

## Filtered prompt counts for (N1,N2) thresholds

How many prompts remain if we filter to prompts with at least **N1** positive candidates and **N2** negative candidates?

| Dataset | Prompts total | Kept @ (5,5) | Kept @ (10,10) |
|---|---:|---:|---:|
| mt_mqm | 1180 | 336 | 0 |
| qa_challenge300 | 300 | 0 | 0 |
| qgen_quizdesign | 452 | 0 | 0 |
| summ_frank_cnndm_test | 175 | 0 | 0 |
| summ_gpt3_cnn | 100 | 0 | 0 |
| summ_gpt3_bbc | 100 | 0 | 0 |
| summ_summeval_consistency | 100 | 1 | 0 |
| summ_summeval_fluency | 100 | 8 | 0 |
| summ_summeval_coherence | 100 | 33 | 0 |
| summ_summeval_relevance | 100 | 29 | 0 |

Notes:
- These counts are computed from `figures/nnd/filter_grid.csv` generated by `scripts/nnd_plots.py`.
- (10,10) is impossible for any dataset whose max candidates/prompt is < 20. In our current set, only MQM has up to 17 and SummEval has 16, so (10,10) necessarily yields 0.
- SummEval is broken out into **4 binary-label datasets** (consistency/fluency/coherence/relevance) based on the expert-majority-of-5 rule.

---

## Distributional intuition (POS/NEG ratios)

### MT MQM (WMT21 EN→DE)
- Overall POS share across all (prompt, system) candidates: **0.588**.
- Per-prompt medians: **2 POS**, **1 NEG**.
- Strong variation: prompts can be all-POS, mixed, or all-NEG.

### QA Challenge300
- Overall POS share: **0.689**.
- Typical prompt has 5 candidates; median split is **4 POS / 1 NEG**.

### QGen QuizDesign
- Overall POS share: **0.460**.
- Typical prompt has 6 candidates; median split is **2 POS / 3 NEG**.

### Summarization SummEval (CNNDM subset)
- Source file (public): `model_annotations.aligned.jsonl` (1600 (prompt, candidate) records = 100 prompts × 16 model summaries).
- Prompt grouping: `id` field (looks like `dm-test-<sha1>`; used as **cnndm_id**).
- Candidates per prompt: always **16**.

#### What “POS” and “NEG” mean in SummEval (in this repo)
SummEval provides **human quality ratings** on a 1–5 scale for 4 dimensions:
`{consistency, coherence, fluency, relevance}`.
Each (document id, model summary) has **3 expert ratings** and **5 crowd (turker) ratings**.

For V2G/NND-style use we need a **binary POS/NEG label** per candidate, so in this repo we convert SummEval’s 1–5 scores into POS/NEG as follows.

**Key idea:** POS means *near-perfect by experts* (a “near-positive”); NEG means *not near-perfect* (which can still be pretty good).

**Rule (expert-majority-of-5; non-arbitrary; mirrors NND helper logic):**
For each dimension we build a separate binary-label dataset.
- Use **only the 3 expert annotations** (`expert_annotations`).
- Let `num_5 = # of experts who gave score 5` for that dimension.
- **POS** iff `num_5 >= 2` (i.e., a strict majority of experts rated it **5/5**).
- **NEG** otherwise (this includes candidates with many 4/5s, mixed scores, etc.).

So **NEG does *not* mean “bad”**; it means “not in the expert-near-perfect bucket”, which is exactly what we want for NND-style near-negative contrast sets.

This yields four datasets:
- `summ_summeval_consistency`
- `summ_summeval_coherence`
- `summ_summeval_fluency`
- `summ_summeval_relevance`

**Why this rule:** it avoids a hand-chosen numeric cutoff (like mean≥4.0) and cleanly operationalizes “near-positive” as *expert consensus on the maximum score*.

- Resulting global stats (under this rule) **depend strongly on the dimension** (100 prompts × 16 candidates = 1600 records):
  - **Consistency:** POS share **0.889**; median per-prompt **14 POS / 2 NEG**
  - **Fluency:** POS share **0.862**; median per-prompt **14 POS / 2 NEG**
  - **Coherence:** POS share **0.244**; median per-prompt **3 POS / 13 NEG**
  - **Relevance:** POS share **0.223**; median per-prompt **3 POS / 13 NEG**

This is the main reason the dimensions “behave very differently” for V2G filtering: some dimensions are POS-heavy (few NEGs), others are NEG-heavy (few POS).

### Summarization FRANK (CNNDM subset)
- Data comes from `https://github.com/artidoro/frank` (raw GitHub files).
- In NND construction, grouping key is **`hash`** (a document-level id); we filter to **CNNDM** items (the NND code uses `origin=="cnndm"`, operationalized as `len(hash) >= 40`).
- Each prompt/hash has exactly **5 model summaries** (candidates).
- Overall POS share (candidate-level): **0.563**.
- Typical prompt (median) has **3 POS / 2 NEG**.

---

## MQM error categories (MT) — what “NEG” looks like

For MT MQM, we labeled system-candidates as:
- **POS** iff *all* annotations for that system on that segment have:
  - `category == No-error` **and** `severity == No-error`
- **NEG** otherwise

Top MQM categories observed across system-candidates (not mutually exclusive):
- `No-error`
- `Style/Awkward`
- `Accuracy/Mistranslation`
- `Fluency/Punctuation`
- `Fluency/Grammar`
- `Terminology/Inappropriate for context`

Top severities:
- `No-error`
- `Minor`
- `Major`

**Data hygiene note:** there are a small number of malformed severity entries in the raw TSV (likely a parsing / data issue). The conservative POS rule above avoids relying on those strings for positives.

---

## Concrete examples (prompt → many completions)

### Example A — 17 candidates, all POS
Prompt/source:
> When he refused, the officials tipped his cart over, destroying all the eggs, the boy alleged.

This segment has **17 systems**; **POS=17, NEG=0**.
Sample completions:
- POS hyp.Facebook-AI: “Als er sich weigerte, kippten die Beamten seinen Wagen um und zerstörten alle Eier, so der Junge.”
- POS hyp.HuaweiTSC: “Als er sich weigerte, kippten die Beamten seinen Wagen um und zerstörten alle Eier, behauptete der Junge.”
- POS hyp.Nemo: “Als er sich weigerte, kippten die Beamten seinen Wagen um und zerstörten alle Eier, behauptete der Junge.”

### Example B — 17 candidates, mixed
Prompt/source:
> Flat, Free Education For Indore Egg Seller Paras Raykar Whose Cart Was Overturned Allegedly Over Rs. 100 Bribe

This segment has **17 systems**; **POS=5, NEG=12**.
Sample completions:
- POS hyp.Online-W: “Freie Bildung für den Eierverkäufer Paras Raykar aus Indore, dessen Karren angeblich wegen 100 Rupien Bestechungsgeld umgeworfen wurde”
- POS ref.A: “Eine Wohnung und kostenlose Ausbildung für den Eierverkäufer Paras Raykar aus Indore, dessen Karren angeblich wegen 100 Rs Schmiergeld umgekippt wurde”
- NEG hyp.Facebook-AI: “*Flache*, kostenlose Bildung …” (Style/Awkward, Terminology/Inappropriate for context)
- NEG hyp.HuaweiTSC: contains untranslated/incorrect phrase (Accuracy/Mistranslation)

### Example C — 17 candidates, all NEG
Prompt/source:
> A roast of former press secretary Sarah Huckabee Sanders by comedian Michelle Wolf … was so vicious that even MSNBC host Mika Brzezinski called it “deplorable.”

This segment has **17 systems**; **POS=0, NEG=17**.
Sample completions:
- NEG hyp.Facebook-AI: uses “Braten” for “roast” (Terminology/Inappropriate for context; Major)
- NEG hyp.Online-W: similar issue plus fluency/punctuation flags

---

## What NND includes vs what is still pending

The NND paper/repo covers multiple domains. From what we can access in this environment, we currently have stats for:
- MT (MQM WMT21 EN→DE) ✅
- QA (Challenge300) ✅
- QGen (QuizDesign) ✅
- Summarization (GPT‑3-era human annotations for CNN/BBC) ✅

Still pending:
- SummEval *paired/scored* variants referenced in some repos (`model_annotations.aligned.scored.jsonl`, `...paired.jsonl`) appear to be **permissioned (HTTP 403)** from the original GCS bucket at time of download. We used the public `model_annotations.aligned.jsonl` instead.
- `keyword_human.json` parsing (schema differs from cnn/bbc)

---

## Repro / scripts

Stats in this note were computed via small Python scripts executed in the OpenClaw environment, using only standard library parsing (no numpy/pandas/datasets installed).

- FRANK fetch + stats script:
  - `Projects/v2g/datasets/candidates/nnd_data/scripts/frank_fetch_and_stats.py`
  - Downloads into: `Projects/v2g/datasets/candidates/nnd_data/frank/`
