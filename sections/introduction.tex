% Introduction

\todo{Draft introduction}

Large language models (LLMs) can be used in two complementary modes: as \emph{generators} that produce candidate answers, and as \emph{validators} that assess answer correctness. Recent work has shown a consistent \emph{generator-validator gap}---models are often better at validating correct answers than generating them \citep{west2022symbolic}.

This gap suggests an opportunity: if a model's validator ``knows'' more than its generator, can we transfer this knowledge to improve generation? We investigate \emph{validator-to-generator} (V2G) training, where we fine-tune generators to produce completions that would rank highly according to the model's own validator.

Our key contributions are:
\begin{itemize}
    \item We identify methodological issues in prior work on ranking alignment, particularly the comparison of pairs across different prompts.
    \item We propose a same-prompt training constraint that ensures valid preference comparisons.
    \item We demonstrate improvements on hypernymy, instruction-following (COLLIE), and question-answering tasks.
\end{itemize}
