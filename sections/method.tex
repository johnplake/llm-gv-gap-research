% Method

\subsection{Problem Setup}

Given a prompt $x$, a generator $\G$ produces completions $y \sim \G(\cdot | x)$, while a validator $\V$ scores completions via $\V(y | x)$. We aim to train $\G$ such that its samples rank highly under $\V$.

\subsection{Same-Prompt Constraint}

A key insight is that preference comparisons must be made \emph{within the same prompt}. Comparing $(x_1, y_1^+)$ vs $(x_2, y_2^-)$ conflates prompt difficulty with answer quality. We enforce:
\begin{equation}
    \mathcal{D} = \{(x, y^+, y^-) : \V(y^+ | x) > \V(y^- | x)\}
\end{equation}
where all comparisons share the same $x$.

\subsection{Training Objective}

We combine ranking loss with NLL regularization to prevent degenerate solutions:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{rank}} + \lambda_G \cdot \text{NLL}_{\text{gen}} + \lambda_V \cdot \text{NLL}_{\text{val}}
\end{equation}

The NLL terms prevent the model from collapsing to ``yes to everything'' validation or losing generation fluency.

\subsection{Typicality Correction}

To account for completion frequency, we subtract unconditional log probability:
\begin{equation}
    \text{score}_{\text{adj}}(y|x) = \text{score}(y|x) - \alpha \cdot \log P_{\text{GPT-2}}(y)
\end{equation}
